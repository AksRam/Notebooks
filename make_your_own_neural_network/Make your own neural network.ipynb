{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network in python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, inputNodes, outputNodes, hiddenNodes, learningRate):\n",
    "        self.inodes = inputNodes\n",
    "        self.onodes = outputNodes\n",
    "        self.hnodes = hiddenNodes\n",
    "        self.lrate = learningRate\n",
    "        \n",
    "        # creating weights matrices\n",
    "        self.wih = (np.random.normal(0.0,pow(self.hnodes,-0/5),(self.hnodes,self.inodes)))\n",
    "        self.who = (np.random.normal(0.0,pow(self.onodes,-0.5),(self.onodes,self.hnodes)))\n",
    "        '''print(\"weights --> hidden-output\")\n",
    "        print(self.who)\n",
    "        print(\"weights --> input-hidden\")\n",
    "        print(self.wih)'''\n",
    "        \n",
    "        #defining the activation function\n",
    "        \n",
    "        self.activationFunction = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "    def train(self, inputList, targetList):\n",
    "        inputs = np.array(inputList,ndmin=2).T\n",
    "        targets = np.array(targetList,ndmin=2).T\n",
    "        \n",
    "        hiddenInputs = np.dot(self.wih, inputs)\n",
    "        hiddenOutputs = self.activationFunction(hiddenInputs)\n",
    "        \n",
    "        finalInputs = np.dot(self.who,hiddenOutputs)\n",
    "        finalOutputs = self.activationFunction(finalInputs)\n",
    "        \n",
    "        outputErrors = (targets - finalOutputs)\n",
    "        #print(\"Ouptut Errors  --> \")\n",
    "        #print(outputErrors)\n",
    "        \n",
    "        hiddenErrors = np.dot(self.who.T,outputErrors)\n",
    "        #print(\"Hidden Errors --> \")\n",
    "        #print(hiddenErrors)\n",
    "        \n",
    "        #update the weights hidden-output\n",
    "        self.who += self.lrate * np.dot((outputErrors*finalOutputs*(1.0-finalOutputs)),np.transpose(hiddenOutputs))\n",
    "        \n",
    "        #update the weights input-hidden\n",
    "        self.wih += self.lrate * np.dot((hiddenErrors*hiddenOutputs*(1.0-hiddenOutputs)),np.transpose(inputs))\n",
    "        \n",
    "        '''print(\"weights --> hidden-output\")\n",
    "        print(self.who)\n",
    "        print(\"weights --> input-hidden\")\n",
    "        print(self.wih)'''\n",
    "        pass\n",
    "    \n",
    "    def query(self, inputList):\n",
    "        #the input matrix\n",
    "        inputs = np.array(inputList,ndmin=2).T\n",
    "        #print(\"Input matrix : \")\n",
    "        #print(inputs)\n",
    "        \n",
    "        #Calculating the hidden layer input matrix \n",
    "        hiddenInputs = np.dot(self.wih,inputs)\n",
    "        #print(\"Hidden layer inputs : \")\n",
    "        #print(hiddenInputs)\n",
    "        #Passing through activation function\n",
    "        hiddenOutputs = self.activationFunction(hiddenInputs)\n",
    "        #print(\"Activated by sigmoid : \")\n",
    "        #print(hiddenOutputs)\n",
    "        \n",
    "        #Caluclating the output layer input matrix\n",
    "        finalInputs = np.dot(self.who,hiddenOutputs)\n",
    "        print(\"final Inputs : \")\n",
    "        print(finalInputs)\n",
    "        #Caluclating final layer output\n",
    "        finalOutputs = self.activationFunction(finalInputs)\n",
    "        \n",
    "        return finalOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of input, output and hidden nodes\n",
    "\n",
    "inputNodes = 3\n",
    "outputNodes = 3\n",
    "hiddenNodes = 3\n",
    "\n",
    "# specify the learning rate\n",
    "\n",
    "learningRate = 0.5\n",
    "\n",
    "# creting a instance of NeuralNetwork\n",
    "\n",
    "n = NeuralNet(inputNodes, outputNodes, hiddenNodes, learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix : \n",
      "[[ 0.2]\n",
      " [ 0.3]\n",
      " [ 0.4]]\n",
      "Hidden layer inputs : \n",
      "[[-0.18562458]\n",
      " [ 0.01477796]\n",
      " [-0.21346983]]\n",
      "Activated by sigmoid : \n",
      "[[ 0.45372665]\n",
      " [ 0.50369442]\n",
      " [ 0.44683428]]\n",
      "final Inputs : \n",
      "[[-0.39196723]\n",
      " [ 0.23424241]\n",
      " [ 0.40036544]]\n",
      "final output : \n",
      "[[ 0.40324382]\n",
      " [ 0.5582943 ]\n",
      " [ 0.59877546]]\n"
     ]
    }
   ],
   "source": [
    "outs = n.query([0.2,0.3,0.4])\n",
    "print(\"final output : \")\n",
    "print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouptut Errors  --> \n",
      "[[-0.00324382]\n",
      " [-0.0582943 ]\n",
      " [ 0.00122454]]\n",
      "Hidden Errors --> \n",
      "[[-0.01017559]\n",
      " [-0.01889328]\n",
      " [ 0.0050133 ]]\n",
      "weights --> hidden-output\n",
      "[[-0.31937084 -0.26262589 -0.2574428 ]\n",
      " [ 0.19005895  0.34201586 -0.06490618]\n",
      " [ 0.04781706  0.33005318  0.47561388]]\n",
      "weights --> input-hidden\n",
      "[[-0.46448623 -0.49794963  0.14072962]\n",
      " [ 0.19544206  0.24204809 -0.24402431]\n",
      " [ 0.40154682 -0.39660077 -0.43654822]]\n"
     ]
    }
   ],
   "source": [
    "n.train([0.2,0.3,0.4],[0.4,0.5,0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouptut Errors  --> \n",
      "[[-0.00319694]\n",
      " [-0.05710606]\n",
      " [ 0.00121079]]\n",
      "Hidden Errors --> \n",
      "[[-0.00977461]\n",
      " [-0.01829196]\n",
      " [ 0.00510543]]\n",
      "weights --> hidden-output\n",
      "[[-0.31954532 -0.26281956 -0.25761469]\n",
      " [ 0.18686302  0.33846847 -0.06805451]\n",
      " [ 0.04788304  0.33012641  0.47567888]]\n",
      "weights --> input-hidden\n",
      "[[-0.46472849 -0.49831302  0.14024509]\n",
      " [ 0.19498478  0.24136217 -0.24493886]\n",
      " [ 0.40167301 -0.39641148 -0.43629583]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00315032]\n",
      " [-0.05594155]\n",
      " [ 0.00119674]]\n",
      "Hidden Errors --> \n",
      "[[-0.00938943]\n",
      " [-0.01771141]\n",
      " [ 0.00518791]]\n",
      "weights --> hidden-output\n",
      "[[-0.31971722 -0.26301034 -0.25778408]\n",
      " [ 0.1837312   0.3349927  -0.07114059]\n",
      " [ 0.04794824  0.33019877  0.47574312]]\n",
      "weights --> input-hidden\n",
      "[[-0.4649612  -0.49866209  0.13977967]\n",
      " [ 0.19454202  0.24069803 -0.24582439]\n",
      " [ 0.40180125 -0.39621913 -0.43603936]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00310398]\n",
      " [-0.05480031]\n",
      " [ 0.00118243]]\n",
      "Hidden Errors --> \n",
      "[[-0.00901944]\n",
      " [-0.01715089]\n",
      " [ 0.00526122]]\n",
      "weights --> hidden-output\n",
      "[[-0.31988656 -0.26319825 -0.25795099]\n",
      " [ 0.18066225  0.33158719 -0.07416557]\n",
      " [ 0.04801264  0.33027024  0.47580661]]\n",
      "weights --> input-hidden\n",
      "[[-0.46518473 -0.49899739  0.1393326 ]\n",
      " [ 0.19411326  0.24005489 -0.2466819 ]\n",
      " [ 0.4019313  -0.39602405 -0.43577925]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00305793]\n",
      " [-0.05368191]\n",
      " [ 0.00116789]]\n",
      "Hidden Errors --> \n",
      "[[-0.00866403]\n",
      " [-0.01660967]\n",
      " [ 0.00532583]]\n",
      "weights --> hidden-output\n",
      "[[-0.32005335 -0.26338331 -0.25811543]\n",
      " [ 0.177655    0.32825057 -0.07713058]\n",
      " [ 0.04807624  0.33034081  0.47586932]]\n",
      "weights --> input-hidden\n",
      "[[-0.46539945 -0.49931947  0.13890317]\n",
      " [ 0.19369804  0.23943205 -0.24751236]\n",
      " [ 0.40206295 -0.39582657 -0.43551595]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.0030122 ]\n",
      " [-0.05258591]\n",
      " [ 0.00115314]]\n",
      "Hidden Errors --> \n",
      "[[-0.00832265]\n",
      " [-0.01608706]\n",
      " [ 0.00538222]]\n",
      "weights --> hidden-output\n",
      "[[-0.32021761 -0.26356553 -0.25827743]\n",
      " [ 0.17470826  0.32498151 -0.08003673]\n",
      " [ 0.04813903  0.33041046  0.47593124]]\n",
      "weights --> input-hidden\n",
      "[[-0.4656057  -0.49962884  0.13849066]\n",
      " [ 0.19329587  0.23882881 -0.24831668]\n",
      " [ 0.402196   -0.395627   -0.43524986]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00296681]\n",
      " [-0.0515119 ]\n",
      " [ 0.00113821]]\n",
      "Hidden Errors --> \n",
      "[[-0.00799474]\n",
      " [-0.01558239]\n",
      " [ 0.00543081]]\n",
      "weights --> hidden-output\n",
      "[[-0.32037936 -0.26374495 -0.258437  ]\n",
      " [ 0.17182087  0.3217787  -0.08288512]\n",
      " [ 0.048201    0.33047919  0.47599236]]\n",
      "weights --> input-hidden\n",
      "[[-0.46580382 -0.49992602  0.13809442]\n",
      " [ 0.19290633  0.23824449 -0.24909578]\n",
      " [ 0.40233025 -0.39542563 -0.43498136]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00292176]\n",
      " [-0.05045944]\n",
      " [ 0.00112313]]\n",
      "Hidden Errors --> \n",
      "[[-0.00767978]\n",
      " [-0.015095  ]\n",
      " [ 0.00547203]]\n",
      "weights --> hidden-output\n",
      "[[-0.32053862 -0.2639216  -0.25859415]\n",
      " [ 0.16899169  0.31864085 -0.08567682]\n",
      " [ 0.04826213  0.330547    0.47605269]]\n",
      "weights --> input-hidden\n",
      "[[-0.46599413 -0.50021149  0.1377138 ]\n",
      " [ 0.19252896  0.23767844 -0.24985051]\n",
      " [ 0.40246552 -0.39522272 -0.43471081]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00287708]\n",
      " [-0.04942813]\n",
      " [ 0.00110792]]\n",
      "Hidden Errors --> \n",
      "[[-0.00737726]\n",
      " [-0.01462428]\n",
      " [ 0.00550627]]\n",
      "weights --> hidden-output\n",
      "[[-0.32069542 -0.26409548 -0.25874891]\n",
      " [ 0.1662196   0.31556668 -0.0884129 ]\n",
      " [ 0.04832242  0.33061386  0.4761122 ]]\n",
      "weights --> input-hidden\n",
      "[[-0.46617694 -0.5004857   0.13734819]\n",
      " [ 0.19216336  0.23713004 -0.2505817 ]\n",
      " [ 0.40260164 -0.39501853 -0.43443857]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00283277]\n",
      " [-0.04841757]\n",
      " [ 0.0010926 ]]\n",
      "Hidden Errors --> \n",
      "[[-0.0070867 ]\n",
      " [-0.01416962]\n",
      " [ 0.00553391]]\n",
      "weights --> hidden-output\n",
      "[[-0.32084978 -0.26426664 -0.2589013 ]\n",
      " [ 0.16350349  0.31255496 -0.0910944 ]\n",
      " [ 0.04838187  0.33067978  0.47617089]]\n",
      "weights --> input-hidden\n",
      "[[-0.46635255 -0.50074911  0.13699698]\n",
      " [ 0.19180913  0.23659869 -0.25129017]\n",
      " [ 0.40273845 -0.39481332 -0.43416495]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    n.train([0.2,0.3,0.4],[0.4,0.5,0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "data_file = open('mnist_train_100.csv','r')\n",
    "data_list = data_file.readlines()\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fca5ab5c450>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADpdJREFUeJzt3X2MVGWWx/HfkRl8ASWiLUEHbRZx40tis6mQTYZs2Iwz\nQZ0EiS+BqGEMkQkRdcz4FoxZYzSRdWcQ4mpsFiKss8xsGIz8YdZRshEnGSeW4Iro7upiI3SQLiJk\nHI0ODWf/6OukR7ueKqpu1a3u8/0kna665z59Twp+favuU12PubsAxHNS0Q0AKAbhB4Ii/EBQhB8I\nivADQRF+ICjCDwRF+IGgCD8Q1LfaebCzzz7bu7u723lIIJS+vj4dOnTI6tm3qfCb2TxJqyWNk/Qv\n7v5Yav/u7m6Vy+VmDgkgoVQq1b1vw0/7zWycpH+WdKWkSyQtMrNLGv15ANqrmdf8syV94O573P1P\nkn4paX4+bQFotWbCf56kfcPu78+2/QUzW2pmZTMrVyqVJg4HIE8tv9rv7r3uXnL3UldXV6sPB6BO\nzYS/X9K0Yfe/k20DMAo0E/43JM00s+lmNl7SQklb82kLQKs1PNXn7oNmtlzSSxqa6lvv7rtz6wxA\nSzU1z+/uL0p6MadeALQRb+8FgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKi2LtGNsWffvn3J+urVq6vWVq1alRx7\n1113Jet33nlnsj5t2rRkPTrO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVFPz/GbWJ+lTScckDbp7\nKY+m0Dn6+/uT9VmzZiXrR44cqVozs+TYJ554IlnfsGFDsl6pVJL16PJ4k8/fu/uhHH4OgDbiaT8Q\nVLPhd0m/MbM3zWxpHg0BaI9mn/bPcfd+MztH0stm9t/uvn34DtkvhaWSdP755zd5OAB5aerM7+79\n2fcBSc9Lmj3CPr3uXnL3UldXVzOHA5CjhsNvZhPM7PSvbkv6gaR38moMQGs187R/iqTns+mab0n6\nN3f/j1y6AtByDYff3fdIujzHXlCAvXv3Jutz585N1g8fPpysp+byJ02alBx78sknJ+sDAwPJ+p49\ne6rWLrjgguTYcePGJetjAVN9QFCEHwiK8ANBEX4gKMIPBEX4gaD46O4x4OjRo1Vrtaby5s2bl6zX\n+mjuZvT09CTrjz76aLI+Z86cZH3mzJlVa729vcmxS5YsSdbHAs78QFCEHwiK8ANBEX4gKMIPBEX4\ngaAIPxAU8/xjwD333FO19uSTT7axkxPz6quvJuufffZZsr5gwYJkfcuWLVVrO3fuTI6NgDM/EBTh\nB4Ii/EBQhB8IivADQRF+ICjCDwTFPP8oUOtv6p977rmqNXdv6ti15tKvvfbaZP2mm26qWps2bVpy\n7MUXX5ys33fffcn65s2bq9aafVzGAs78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxCU1ZrvNLP1kn4o\nacDdL8u2TZb0K0ndkvok3eDu6bWaJZVKJS+Xy022PPb09/cn65dfnl4J/ciRIw0f+8Ybb0zW165d\nm6y/++67yfqOHTuq1hYuXJgce9pppyXrtaSW2Z4wYUJy7O7du5P1Wu9RKEqpVFK5XK6+Lvow9Zz5\nn5X09ZUd7pe0zd1nStqW3QcwitQMv7tvl/TJ1zbPl7Qhu71B0jU59wWgxRp9zT/F3Q9ktz+WNCWn\nfgC0SdMX/HzookHVCwdmttTMymZWrlQqzR4OQE4aDf9BM5sqSdn3gWo7unuvu5fcvdTV1dXg4QDk\nrdHwb5W0OLu9WNIL+bQDoF1qht/MNkn6naS/NrP9ZrZE0mOSvm9m70u6IrsPYBSp+ff87r6oSul7\nOfcyZh06dChZX7lyZbJ++HD6LRRTplS/3jp9+vTk2GXLliXr48ePT9Z7enqaqhfl888/T9Yff/zx\nZH3NmjV5tlMI3uEHBEX4gaAIPxAU4QeCIvxAUIQfCIqP7s7B4OBgsn733Xcn66mP3pakSZMmJesv\nvfRS1dqFF16YHHv06NFkPaoPP/yw6BZajjM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFPH8OPvro\no2S91jx+La+//nqyftFFFzX8s0899dSGx2J048wPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exz5+D\n2267LVmvtQz6ggULkvVm5vEjO378eNXaSSelz3u1/s3GAs78QFCEHwiK8ANBEX4gKMIPBEX4gaAI\nPxBUzXl+M1sv6YeSBtz9smzbQ5JulVTJdlvh7i+2qslOsHPnzqq17du3J8eaWbJ+/fXXN9QT0lJz\n+bX+TUqlUt7tdJx6zvzPSpo3wvZV7t6TfY3p4ANjUc3wu/t2SZ+0oRcAbdTMa/7lZva2ma03szNz\n6whAWzQa/qclzZDUI+mApJ9V29HMlppZ2czKlUql2m4A2qyh8Lv7QXc/5u7HJa2VNDuxb6+7l9y9\n1NXV1WifAHLWUPjNbOqwuwskvZNPOwDapZ6pvk2S5ko628z2S/oHSXPNrEeSS+qT9OMW9gigBWqG\n390XjbB5XQt66WhffPFF1dqXX36ZHHvuuecm61dffXVDPY11g4ODyfqaNWsa/tnXXXddsr5ixYqG\nf/ZowTv8gKAIPxAU4QeCIvxAUIQfCIrwA0Hx0d1tcMoppyTrEydObFMnnaXWVN7TTz+drN97773J\nend3d9XaAw88kBw7fvz4ZH0s4MwPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exz98GN998c9EtFKa/\nv79qbeXKlcmxTz31VLJ+yy23JOtr165N1qPjzA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQTHPXyd3\nb6gmSc8++2yy/uCDDzbSUkfYtGlTsn777bdXrR0+fDg59o477kjWV61alawjjTM/EBThB4Ii/EBQ\nhB8IivADQRF+ICjCDwRVc57fzKZJ2ihpiiSX1Ovuq81ssqRfSeqW1CfpBndPT9yOYmbWUE2S9u/f\nn6w//PDDyfqSJUuS9dNPP71qbffu3cmxzzzzTLL+2muvJet9fX3J+owZM6rWFi5cmBxba54fzann\nzD8o6afufomkv5V0m5ldIul+Sdvcfaakbdl9AKNEzfC7+wF335Hd/lTSe5LOkzRf0oZstw2SrmlV\nkwDyd0Kv+c2sW9IsSb+XNMXdD2SljzX0sgDAKFF3+M1soqRfS/qJu/9heM2H3tw+4hvczWypmZXN\nrFypVJpqFkB+6gq/mX1bQ8H/hbtvyTYfNLOpWX2qpIGRxrp7r7uX3L3U1dWVR88AclAz/DZ0KXud\npPfc/efDSlslLc5uL5b0Qv7tAWiVev6k97uSbpa0y8zeyratkPSYpH83syWS9kq6oTUtjn7Hjh1L\n1mtN9a1bty5Znzx5ctXarl27kmObdeWVVybr8+bNq1pbvnx53u3gBNQMv7v/VlK1iezv5dsOgHbh\nHX5AUIQfCIrwA0ERfiAowg8ERfiBoPjo7jpdeumlVWtXXHFFcuwrr7zS1LFr/UlwahnsWs4555xk\nfdmyZcn6aP7Y8eg48wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUMzz1+mMM86oWtu8eXNy7MaNG5P1\nVn5E9SOPPJKs33rrrcn6WWedlWc76CCc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKBtaaas9SqWS\nl8vlth0PiKZUKqlcLqfXjM9w5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoGqG38ymmdl/mtm7Zrbb\nzO7Mtj9kZv1m9lb2dVXr2wWQl3o+zGNQ0k/dfYeZnS7pTTN7Oautcvd/al17AFqlZvjd/YCkA9nt\nT83sPUnntboxAK11Qq/5zaxb0ixJv882LTezt81svZmdWWXMUjMrm1m5Uqk01SyA/NQdfjObKOnX\nkn7i7n+Q9LSkGZJ6NPTM4GcjjXP3XncvuXupq6srh5YB5KGu8JvZtzUU/F+4+xZJcveD7n7M3Y9L\nWitpduvaBJC3eq72m6R1kt5z958P2z512G4LJL2Tf3sAWqWeq/3flXSzpF1m9la2bYWkRWbWI8kl\n9Un6cUs6BNAS9Vzt/62kkf4++MX82wHQLrzDDwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQf\nCIrwA0ERfiAowg8ERfiBoAg/EFRbl+g2s4qkvcM2nS3pUNsaODGd2lun9iXRW6Py7O0Cd6/r8/La\nGv5vHNys7O6lwhpI6NTeOrUvid4aVVRvPO0HgiL8QFBFh7+34OOndGpvndqXRG+NKqS3Ql/zAyhO\n0Wd+AAUpJPxmNs/M/sfMPjCz+4vooRoz6zOzXdnKw+WCe1lvZgNm9s6wbZPN7GUzez/7PuIyaQX1\n1hErNydWli70seu0Fa/b/rTfzMZJ+l9J35e0X9Ibkha5+7ttbaQKM+uTVHL3wueEzezvJP1R0kZ3\nvyzb9o+SPnH3x7JfnGe6+30d0ttDkv5Y9MrN2YIyU4evLC3pGkk/UoGPXaKvG1TA41bEmX+2pA/c\nfY+7/0nSLyXNL6CPjufu2yV98rXN8yVtyG5v0NB/nrar0ltHcPcD7r4ju/2ppK9Wli70sUv0VYgi\nwn+epH3D7u9XZy357ZJ+Y2ZvmtnSopsZwZRs2XRJ+ljSlCKbGUHNlZvb6WsrS3fMY9fIitd544Lf\nN81x97+RdKWk27Kntx3Jh16zddJ0TV0rN7fLCCtL/1mRj12jK17nrYjw90uaNuz+d7JtHcHd+7Pv\nA5KeV+etPnzwq0VSs+8DBffzZ520cvNIK0urAx67TlrxuojwvyFppplNN7PxkhZK2lpAH99gZhOy\nCzEyswmSfqDOW314q6TF2e3Fkl4osJe/0CkrN1dbWVoFP3Ydt+K1u7f9S9JVGrri/3+SHiiihyp9\n/ZWk/8q+dhfdm6RNGnoaeFRD10aWSDpL0jZJ70t6RdLkDurtXyXtkvS2hoI2taDe5mjoKf3bkt7K\nvq4q+rFL9FXI48Y7/ICguOAHBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo/wfNDnvJ0xlPmwAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca5ac9a250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "all_values = data_list[1].split(',')\n",
    "image_array = np.asfarray(all_values[1:]).reshape(28,28)\n",
    "matplotlib.pyplot.imshow(image_array,cmap='Greys',interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.208       0.62729412  0.99223529  0.62729412  0.20411765\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.19635294  0.934       0.98835294  0.98835294  0.98835294\n",
      "  0.93011765  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.21964706  0.89129412  0.99223529  0.98835294  0.93788235\n",
      "  0.91458824  0.98835294  0.23129412  0.03329412  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.04882353  0.24294118  0.87964706  0.98835294  0.99223529  0.98835294\n",
      "  0.79423529  0.33611765  0.98835294  0.99223529  0.48364706  0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.64282353  0.98835294  0.98835294  0.98835294  0.99223529\n",
      "  0.98835294  0.98835294  0.38270588  0.74376471  0.99223529  0.65835294\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.208       0.934       0.99223529  0.99223529\n",
      "  0.74764706  0.45258824  0.99223529  0.89517647  0.19247059  0.31670588\n",
      "  1.          0.66223529  0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.19635294  0.934       0.98835294\n",
      "  0.98835294  0.70494118  0.05658824  0.30117647  0.47976471  0.09152941\n",
      "  0.01        0.01        0.99223529  0.95341176  0.20411765  0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.15752941  0.65058824\n",
      "  0.99223529  0.91458824  0.81752941  0.33611765  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.99223529  0.98835294  0.65058824\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.03717647\n",
      "  0.70105882  0.98835294  0.94176471  0.28564706  0.08376471  0.11870588\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.99223529  0.98835294  0.76705882  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.23129412  0.98835294  0.98835294  0.25458824  0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.99223529  0.98835294  0.76705882  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.77870588  0.99223529  0.74764706  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  1.          0.99223529  0.77094118  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.30505882  0.96505882  0.98835294  0.44482353  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.99223529  0.98835294  0.58458824  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.34        0.98835294  0.90294118  0.10705882  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.03717647\n",
      "  0.53411765  0.99223529  0.73211765  0.05658824  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.34        0.98835294  0.87576471  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.03717647\n",
      "  0.51858824  0.98835294  0.88352941  0.28564706  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.34        0.98835294  0.57294118  0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.19635294\n",
      "  0.65058824  0.98835294  0.68164706  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.34388235  0.99223529  0.88352941\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.45258824  0.934       0.99223529  0.63894118  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.34\n",
      "  0.98835294  0.97670588  0.57682353  0.19635294  0.12258824  0.34\n",
      "  0.70105882  0.88352941  0.99223529  0.87576471  0.65835294  0.22741176\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.34        0.98835294  0.98835294  0.98835294  0.89905882\n",
      "  0.84470588  0.98835294  0.98835294  0.98835294  0.77094118  0.51470588\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.11870588  0.78258824  0.98835294\n",
      "  0.98835294  0.99223529  0.98835294  0.98835294  0.91458824  0.57294118\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.10705882  0.50694118  0.98835294  0.99223529  0.98835294  0.55741176\n",
      "  0.15364706  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01      ]\n"
     ]
    }
   ],
   "source": [
    "scaled_input = (np.asfarray(all_values[1:])/255.0 * 0.99)+0.01\n",
    "print(scaled_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputNodes = 784\n",
    "outputNodes = 10\n",
    "hiddenNodes = 200\n",
    "\n",
    "\n",
    "learningRate = 0.1\n",
    "\n",
    "nN = NeuralNet(inputNodes, outputNodes, hiddenNodes, learningRate)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "    for record in data_list:\n",
    "        all_vaules = record.split(',')\n",
    "        inputs = (np.asfarray(all_values[1:])/255*0.99)+0.01\n",
    "        targets = np.zeros(outputNodes)+0.01\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        nN.train(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = open('mnist_test_10.csv','r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "all_vals = test_data_list[0].split(',')\n",
    "print(all_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fca5a861d50>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npP4Pc0oGVHoLWbQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca5a8eb610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageArray = np.asfarray(all_vals[1:]).reshape(28,28)\n",
    "matplotlib.pyplot.imshow(imageArray, cmap=\"Greys\",interpolation=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final Inputs : \n",
      "[[-1.36214785]\n",
      " [ 3.27288933]\n",
      " [ 0.96813141]\n",
      " [-1.63563069]\n",
      " [-2.51966246]\n",
      " [-3.62769904]\n",
      " [-3.78545692]\n",
      " [-2.64775507]\n",
      " [-2.11622465]\n",
      " [-1.1031951 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.20389144],\n",
       "       [ 0.96348695],\n",
       "       [ 0.72474689],\n",
       "       [ 0.16306047],\n",
       "       [ 0.07449121],\n",
       "       [ 0.0258892 ],\n",
       "       [ 0.0221947 ],\n",
       "       [ 0.06612751],\n",
       "       [ 0.10752984],\n",
       "       [ 0.24914171]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nN.query((np.asfarray(all_vals[1:])/255.0 * 0.99) +0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
