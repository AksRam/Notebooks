{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network in python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, inputNodes, outputNodes, hiddenNodes, learningRate):\n",
    "        self.inodes = inputNodes\n",
    "        self.onodes = outputNodes\n",
    "        self.hnodes = hiddenNodes\n",
    "        self.lrate = learningRate\n",
    "        \n",
    "        # creating weights matrices\n",
    "        self.wih = (np.random.normal(0.0,pow(self.hnodes,-0/5),(self.hnodes,self.inodes)))\n",
    "        self.who = (np.random.normal(0.0,pow(self.onodes,-0.5),(self.onodes,self.hnodes)))\n",
    "        '''print(\"weights --> hidden-output\")\n",
    "        print(self.who)\n",
    "        print(\"weights --> input-hidden\")\n",
    "        print(self.wih)'''\n",
    "        \n",
    "        #defining the activation function\n",
    "        \n",
    "        self.activationFunction = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "    def train(self, inputList, targetList):\n",
    "        inputs = np.array(inputList,ndmin=2).T\n",
    "        targets = np.array(targetList,ndmin=2).T\n",
    "        \n",
    "        hiddenInputs = np.dot(self.wih, inputs)\n",
    "        hiddenOutputs = self.activationFunction(hiddenInputs)\n",
    "        \n",
    "        finalInputs = np.dot(self.who,hiddenOutputs)\n",
    "        finalOutputs = self.activationFunction(finalInputs)\n",
    "        \n",
    "        outputErrors = (targets - finalOutputs)\n",
    "        #print(\"Ouptut Errors  --> \")\n",
    "        #print(outputErrors)\n",
    "        \n",
    "        hiddenErrors = np.dot(self.who.T,outputErrors)\n",
    "        #print(\"Hidden Errors --> \")\n",
    "        #print(hiddenErrors)\n",
    "        \n",
    "        #update the weights hidden-output\n",
    "        self.who += self.lrate * np.dot((outputErrors*finalOutputs*(1.0-finalOutputs)),np.transpose(hiddenOutputs))\n",
    "        \n",
    "        #update the weights input-hidden\n",
    "        self.wih += self.lrate * np.dot((hiddenErrors*hiddenOutputs*(1.0-hiddenOutputs)),np.transpose(inputs))\n",
    "        \n",
    "        '''print(\"weights --> hidden-output\")\n",
    "        print(self.who)\n",
    "        print(\"weights --> input-hidden\")\n",
    "        print(self.wih)'''\n",
    "        pass\n",
    "    \n",
    "    def query(self, inputList):\n",
    "        #the input matrix\n",
    "        inputs = np.array(inputList,ndmin=2).T\n",
    "        #print(\"Input matrix : \")\n",
    "        #print(inputs)\n",
    "        \n",
    "        #Calculating the hidden layer input matrix \n",
    "        hiddenInputs = np.dot(self.wih,inputs)\n",
    "        #print(\"Hidden layer inputs : \")\n",
    "        #print(hiddenInputs)\n",
    "        #Passing through activation function\n",
    "        hiddenOutputs = self.activationFunction(hiddenInputs)\n",
    "        #print(\"Activated by sigmoid : \")\n",
    "        #print(hiddenOutputs)\n",
    "        \n",
    "        #Caluclating the output layer input matrix\n",
    "        finalInputs = np.dot(self.who,hiddenOutputs)\n",
    "        print(\"final Inputs : \")\n",
    "        print(finalInputs)\n",
    "        #Caluclating final layer output\n",
    "        finalOutputs = self.activationFunction(finalInputs)\n",
    "        \n",
    "        return finalOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of input, output and hidden nodes\n",
    "\n",
    "inputNodes = 3\n",
    "outputNodes = 3\n",
    "hiddenNodes = 3\n",
    "\n",
    "# specify the learning rate\n",
    "\n",
    "learningRate = 0.5\n",
    "\n",
    "# creting a instance of NeuralNetwork\n",
    "\n",
    "n = NeuralNet(inputNodes, outputNodes, hiddenNodes, learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix : \n",
      "[[ 0.2]\n",
      " [ 0.3]\n",
      " [ 0.4]]\n",
      "Hidden layer inputs : \n",
      "[[-0.18562458]\n",
      " [ 0.01477796]\n",
      " [-0.21346983]]\n",
      "Activated by sigmoid : \n",
      "[[ 0.45372665]\n",
      " [ 0.50369442]\n",
      " [ 0.44683428]]\n",
      "final Inputs : \n",
      "[[-0.39196723]\n",
      " [ 0.23424241]\n",
      " [ 0.40036544]]\n",
      "final output : \n",
      "[[ 0.40324382]\n",
      " [ 0.5582943 ]\n",
      " [ 0.59877546]]\n"
     ]
    }
   ],
   "source": [
    "outs = n.query([0.2,0.3,0.4])\n",
    "print(\"final output : \")\n",
    "print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouptut Errors  --> \n",
      "[[-0.00324382]\n",
      " [-0.0582943 ]\n",
      " [ 0.00122454]]\n",
      "Hidden Errors --> \n",
      "[[-0.01017559]\n",
      " [-0.01889328]\n",
      " [ 0.0050133 ]]\n",
      "weights --> hidden-output\n",
      "[[-0.31937084 -0.26262589 -0.2574428 ]\n",
      " [ 0.19005895  0.34201586 -0.06490618]\n",
      " [ 0.04781706  0.33005318  0.47561388]]\n",
      "weights --> input-hidden\n",
      "[[-0.46448623 -0.49794963  0.14072962]\n",
      " [ 0.19544206  0.24204809 -0.24402431]\n",
      " [ 0.40154682 -0.39660077 -0.43654822]]\n"
     ]
    }
   ],
   "source": [
    "n.train([0.2,0.3,0.4],[0.4,0.5,0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouptut Errors  --> \n",
      "[[-0.00319694]\n",
      " [-0.05710606]\n",
      " [ 0.00121079]]\n",
      "Hidden Errors --> \n",
      "[[-0.00977461]\n",
      " [-0.01829196]\n",
      " [ 0.00510543]]\n",
      "weights --> hidden-output\n",
      "[[-0.31954532 -0.26281956 -0.25761469]\n",
      " [ 0.18686302  0.33846847 -0.06805451]\n",
      " [ 0.04788304  0.33012641  0.47567888]]\n",
      "weights --> input-hidden\n",
      "[[-0.46472849 -0.49831302  0.14024509]\n",
      " [ 0.19498478  0.24136217 -0.24493886]\n",
      " [ 0.40167301 -0.39641148 -0.43629583]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00315032]\n",
      " [-0.05594155]\n",
      " [ 0.00119674]]\n",
      "Hidden Errors --> \n",
      "[[-0.00938943]\n",
      " [-0.01771141]\n",
      " [ 0.00518791]]\n",
      "weights --> hidden-output\n",
      "[[-0.31971722 -0.26301034 -0.25778408]\n",
      " [ 0.1837312   0.3349927  -0.07114059]\n",
      " [ 0.04794824  0.33019877  0.47574312]]\n",
      "weights --> input-hidden\n",
      "[[-0.4649612  -0.49866209  0.13977967]\n",
      " [ 0.19454202  0.24069803 -0.24582439]\n",
      " [ 0.40180125 -0.39621913 -0.43603936]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00310398]\n",
      " [-0.05480031]\n",
      " [ 0.00118243]]\n",
      "Hidden Errors --> \n",
      "[[-0.00901944]\n",
      " [-0.01715089]\n",
      " [ 0.00526122]]\n",
      "weights --> hidden-output\n",
      "[[-0.31988656 -0.26319825 -0.25795099]\n",
      " [ 0.18066225  0.33158719 -0.07416557]\n",
      " [ 0.04801264  0.33027024  0.47580661]]\n",
      "weights --> input-hidden\n",
      "[[-0.46518473 -0.49899739  0.1393326 ]\n",
      " [ 0.19411326  0.24005489 -0.2466819 ]\n",
      " [ 0.4019313  -0.39602405 -0.43577925]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00305793]\n",
      " [-0.05368191]\n",
      " [ 0.00116789]]\n",
      "Hidden Errors --> \n",
      "[[-0.00866403]\n",
      " [-0.01660967]\n",
      " [ 0.00532583]]\n",
      "weights --> hidden-output\n",
      "[[-0.32005335 -0.26338331 -0.25811543]\n",
      " [ 0.177655    0.32825057 -0.07713058]\n",
      " [ 0.04807624  0.33034081  0.47586932]]\n",
      "weights --> input-hidden\n",
      "[[-0.46539945 -0.49931947  0.13890317]\n",
      " [ 0.19369804  0.23943205 -0.24751236]\n",
      " [ 0.40206295 -0.39582657 -0.43551595]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.0030122 ]\n",
      " [-0.05258591]\n",
      " [ 0.00115314]]\n",
      "Hidden Errors --> \n",
      "[[-0.00832265]\n",
      " [-0.01608706]\n",
      " [ 0.00538222]]\n",
      "weights --> hidden-output\n",
      "[[-0.32021761 -0.26356553 -0.25827743]\n",
      " [ 0.17470826  0.32498151 -0.08003673]\n",
      " [ 0.04813903  0.33041046  0.47593124]]\n",
      "weights --> input-hidden\n",
      "[[-0.4656057  -0.49962884  0.13849066]\n",
      " [ 0.19329587  0.23882881 -0.24831668]\n",
      " [ 0.402196   -0.395627   -0.43524986]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00296681]\n",
      " [-0.0515119 ]\n",
      " [ 0.00113821]]\n",
      "Hidden Errors --> \n",
      "[[-0.00799474]\n",
      " [-0.01558239]\n",
      " [ 0.00543081]]\n",
      "weights --> hidden-output\n",
      "[[-0.32037936 -0.26374495 -0.258437  ]\n",
      " [ 0.17182087  0.3217787  -0.08288512]\n",
      " [ 0.048201    0.33047919  0.47599236]]\n",
      "weights --> input-hidden\n",
      "[[-0.46580382 -0.49992602  0.13809442]\n",
      " [ 0.19290633  0.23824449 -0.24909578]\n",
      " [ 0.40233025 -0.39542563 -0.43498136]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00292176]\n",
      " [-0.05045944]\n",
      " [ 0.00112313]]\n",
      "Hidden Errors --> \n",
      "[[-0.00767978]\n",
      " [-0.015095  ]\n",
      " [ 0.00547203]]\n",
      "weights --> hidden-output\n",
      "[[-0.32053862 -0.2639216  -0.25859415]\n",
      " [ 0.16899169  0.31864085 -0.08567682]\n",
      " [ 0.04826213  0.330547    0.47605269]]\n",
      "weights --> input-hidden\n",
      "[[-0.46599413 -0.50021149  0.1377138 ]\n",
      " [ 0.19252896  0.23767844 -0.24985051]\n",
      " [ 0.40246552 -0.39522272 -0.43471081]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00287708]\n",
      " [-0.04942813]\n",
      " [ 0.00110792]]\n",
      "Hidden Errors --> \n",
      "[[-0.00737726]\n",
      " [-0.01462428]\n",
      " [ 0.00550627]]\n",
      "weights --> hidden-output\n",
      "[[-0.32069542 -0.26409548 -0.25874891]\n",
      " [ 0.1662196   0.31556668 -0.0884129 ]\n",
      " [ 0.04832242  0.33061386  0.4761122 ]]\n",
      "weights --> input-hidden\n",
      "[[-0.46617694 -0.5004857   0.13734819]\n",
      " [ 0.19216336  0.23713004 -0.2505817 ]\n",
      " [ 0.40260164 -0.39501853 -0.43443857]]\n",
      "Ouptut Errors  --> \n",
      "[[-0.00283277]\n",
      " [-0.04841757]\n",
      " [ 0.0010926 ]]\n",
      "Hidden Errors --> \n",
      "[[-0.0070867 ]\n",
      " [-0.01416962]\n",
      " [ 0.00553391]]\n",
      "weights --> hidden-output\n",
      "[[-0.32084978 -0.26426664 -0.2589013 ]\n",
      " [ 0.16350349  0.31255496 -0.0910944 ]\n",
      " [ 0.04838187  0.33067978  0.47617089]]\n",
      "weights --> input-hidden\n",
      "[[-0.46635255 -0.50074911  0.13699698]\n",
      " [ 0.19180913  0.23659869 -0.25129017]\n",
      " [ 0.40273845 -0.39481332 -0.43416495]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    n.train([0.2,0.3,0.4],[0.4,0.5,0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "data_file = open('mnist_train_100.csv','r')\n",
    "data_list = data_file.readlines()\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fca5ab5c450>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca5ac9a250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "all_values = data_list[1].split(',')\n",
    "image_array = np.asfarray(all_values[1:]).reshape(28,28)\n",
    "matplotlib.pyplot.imshow(image_array,cmap='Greys',interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.208       0.62729412  0.99223529  0.62729412  0.20411765\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.19635294  0.934       0.98835294  0.98835294  0.98835294\n",
      "  0.93011765  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.21964706  0.89129412  0.99223529  0.98835294  0.93788235\n",
      "  0.91458824  0.98835294  0.23129412  0.03329412  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.04882353  0.24294118  0.87964706  0.98835294  0.99223529  0.98835294\n",
      "  0.79423529  0.33611765  0.98835294  0.99223529  0.48364706  0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.64282353  0.98835294  0.98835294  0.98835294  0.99223529\n",
      "  0.98835294  0.98835294  0.38270588  0.74376471  0.99223529  0.65835294\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.208       0.934       0.99223529  0.99223529\n",
      "  0.74764706  0.45258824  0.99223529  0.89517647  0.19247059  0.31670588\n",
      "  1.          0.66223529  0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.19635294  0.934       0.98835294\n",
      "  0.98835294  0.70494118  0.05658824  0.30117647  0.47976471  0.09152941\n",
      "  0.01        0.01        0.99223529  0.95341176  0.20411765  0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.15752941  0.65058824\n",
      "  0.99223529  0.91458824  0.81752941  0.33611765  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.99223529  0.98835294  0.65058824\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.03717647\n",
      "  0.70105882  0.98835294  0.94176471  0.28564706  0.08376471  0.11870588\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.99223529  0.98835294  0.76705882  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.23129412  0.98835294  0.98835294  0.25458824  0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.99223529  0.98835294  0.76705882  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.77870588  0.99223529  0.74764706  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  1.          0.99223529  0.77094118  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.30505882  0.96505882  0.98835294  0.44482353  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.99223529  0.98835294  0.58458824  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.34        0.98835294  0.90294118  0.10705882  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.03717647\n",
      "  0.53411765  0.99223529  0.73211765  0.05658824  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.34        0.98835294  0.87576471  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.03717647\n",
      "  0.51858824  0.98835294  0.88352941  0.28564706  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.34        0.98835294  0.57294118  0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.19635294\n",
      "  0.65058824  0.98835294  0.68164706  0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.34388235  0.99223529  0.88352941\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.45258824  0.934       0.99223529  0.63894118  0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.34\n",
      "  0.98835294  0.97670588  0.57682353  0.19635294  0.12258824  0.34\n",
      "  0.70105882  0.88352941  0.99223529  0.87576471  0.65835294  0.22741176\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.34        0.98835294  0.98835294  0.98835294  0.89905882\n",
      "  0.84470588  0.98835294  0.98835294  0.98835294  0.77094118  0.51470588\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.11870588  0.78258824  0.98835294\n",
      "  0.98835294  0.99223529  0.98835294  0.98835294  0.91458824  0.57294118\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.10705882  0.50694118  0.98835294  0.99223529  0.98835294  0.55741176\n",
      "  0.15364706  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "  0.01      ]\n"
     ]
    }
   ],
   "source": [
    "scaled_input = (np.asfarray(all_values[1:])/255.0 * 0.99)+0.01\n",
    "print(scaled_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputNodes = 784\n",
    "outputNodes = 10\n",
    "hiddenNodes = 200\n",
    "\n",
    "\n",
    "learningRate = 0.1\n",
    "\n",
    "nN = NeuralNet(inputNodes, outputNodes, hiddenNodes, learningRate)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "    for record in data_list:\n",
    "        all_vaules = record.split(',')\n",
    "        inputs = (np.asfarray(all_values[1:])/255*0.99)+0.01\n",
    "        targets = np.zeros(outputNodes)+0.01\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        nN.train(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = open('mnist_test_10.csv','r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "all_vals = test_data_list[0].split(',')\n",
    "print(all_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fca5a932ad0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca5a999cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageArray = np.asfarray(all_vals[1:]).reshape(28,28)\n",
    "matplotlib.pyplot.imshow(imageArray, cmap=\"Greys\",interpolation=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final Inputs : \n",
      "[[-2.72132337]\n",
      " [ 2.77964712]\n",
      " [ 2.28305767]\n",
      " [-1.16303198]\n",
      " [-7.22767106]\n",
      " [-2.5541638 ]\n",
      " [-0.63913143]\n",
      " [-4.80741186]\n",
      " [-1.67117356]\n",
      " [ 3.01558485]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  6.17267768e-02],\n",
       "       [  9.41566032e-01],\n",
       "       [  9.07464129e-01],\n",
       "       [  2.38116795e-01],\n",
       "       [  7.25683199e-04],\n",
       "       [  7.21472551e-02],\n",
       "       [  3.45442908e-01],\n",
       "       [  8.10278338e-03],\n",
       "       [  1.58267775e-01],\n",
       "       [  9.53273253e-01]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nN.query((np.asfarray(all_vals[1:])/255.0 * 0.99) +0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
